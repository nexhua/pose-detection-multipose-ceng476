{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement tensorflow==2.4.1 (from versions: 2.5.0, 2.5.1, 2.5.2, 2.5.3, 2.6.0rc0, 2.6.0rc1, 2.6.0rc2, 2.6.0, 2.6.1, 2.6.2, 2.6.3, 2.6.4, 2.6.5, 2.7.0rc0, 2.7.0rc1, 2.7.0, 2.7.1, 2.7.2, 2.7.3, 2.7.4, 2.8.0rc0, 2.8.0rc1, 2.8.0, 2.8.1, 2.8.2, 2.8.3, 2.8.4, 2.9.0rc0, 2.9.0rc1, 2.9.0rc2, 2.9.0, 2.9.1, 2.9.2, 2.9.3, 2.10.0rc0, 2.10.0rc1, 2.10.0rc2, 2.10.0rc3, 2.10.0, 2.10.1, 2.11.0rc0, 2.11.0rc1, 2.11.0rc2, 2.11.0)\n",
      "ERROR: No matching distribution found for tensorflow==2.4.1\n"
     ]
    }
   ],
   "source": [
    "%pip install tensorflow==2.4.1 tensorflow-gpu==2.4.1 tensorflow-hub opencv-python matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "print(tf.test.is_built_with_gpu_support())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### LOAD MODAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = hub.load(\"./movenet_multipose_lightning_v1/\")\n",
    "movenet = model.signatures['serving_default']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### DRAW KEYPOINTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keypoints from the docs\n",
    "# https://storage.googleapis.com/movenet/MoveNet.MultiPose%20Model%20Card.pdf\n",
    "keypoints = [\"nose\", \"left eye\", \"right eye\", \"left ear\", \"right ear\", \"left shoulder\", \"right shoulder\", \"left elbow\", \"right elbow\", \"left wrist\", \"right wrist\", \"left hip\", \"right hip\", \"left knee\", \"right knee\", \"left ankle\", \"right ankle\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_keypoints(frame, keypoints, confidence_threshold):\n",
    "    y, x, c = frame.shape\n",
    "    shape = np.multiply(keypoints, [y,x,1]).squeeze()\n",
    "\n",
    "    for key_point in shape:\n",
    "        key_point_y, key_point_x, key_point_confidence = key_point\n",
    "        if key_point_confidence > confidence_threshold:\n",
    "            cv2.circle(frame, (int(key_point_x),int(key_point_y)), 6, (0, 0, 255), -1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### DRAW EDGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edges from the docs\n",
    "# https://storage.googleapis.com/movenet/MoveNet.MultiPose%20Model%20Card.pdf\n",
    "edges = {\n",
    "    (0, 1): 'm',\n",
    "    (0, 2): 'c',\n",
    "    (1, 3): 'm',\n",
    "    (2, 4): 'c',\n",
    "    (0, 5): 'm',\n",
    "    (0, 6): 'c',\n",
    "    (5, 7): 'm',\n",
    "    (7, 9): 'm',\n",
    "    (6, 8): 'c',\n",
    "    (8, 10): 'c',\n",
    "    (5, 6): 'y',\n",
    "    (5, 11): 'm',\n",
    "    (6, 12): 'c',\n",
    "    (11, 12): 'y',\n",
    "    (11, 13): 'm',\n",
    "    (13, 15): 'm',\n",
    "    (12, 14): 'c',\n",
    "    (14, 16): 'c'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_edge_color(color):\n",
    "    if color == \"m\":\n",
    "        return (255,102,51)\n",
    "    elif color == \"c\":\n",
    "        return (52,235,113)\n",
    "    else:\n",
    "        return (117,50,125) \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_edges(frame, keypoints, edges, confidence_threshold):\n",
    "    y, x, c = frame.shape\n",
    "    shape = np.multiply(keypoints, [y,x,1]).squeeze()\n",
    "\n",
    "    for edge, color in edges.items():\n",
    "        keypoint_1, keypoint_2 = edge\n",
    "        keypoint_1_y, keypoint_1_x, keypoint_1_c = shape[keypoint_1]\n",
    "        keypoint_2_y, keypoint_2_x, keypoint_2_c = shape[keypoint_2]\n",
    "        if((keypoint_1_c > confidence_threshold) & (keypoint_2_c > confidence_threshold)):\n",
    "            cv2.line(frame, (int(keypoint_1_x), int(keypoint_1_y)), (int(keypoint_2_x), int(keypoint_2_y)), get_edge_color(color), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_keypoints_and_edges(frame, keypoints_and_scores, confidence_threshold):\n",
    "    for person in keypoints_and_scores:\n",
    "        draw_edges(frame, person, edges, confidence_threshold)\n",
    "        draw_keypoints(frame, person, confidence_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_aspect_ratio_resizer(image, target_size):\n",
    "\n",
    "  height, width, _ = image.shape\n",
    "  if height > width:\n",
    "    scale = float(target_size / height)\n",
    "    target_height = target_size\n",
    "    scaled_width = math.ceil(width * scale)\n",
    "    target_width = int(math.ceil(scaled_width / 32) * 32)\n",
    "  else:\n",
    "    scale = float(target_size / width)\n",
    "    target_width = target_size\n",
    "    scaled_height = math.ceil(height * scale)\n",
    "    target_height = int(math.ceil(scaled_height / 32) * 32)\n",
    "  return target_height, target_width"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PREDICTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "capture = cv2.VideoCapture(\"./people_dancing.mp4\")\n",
    "\n",
    "# Documentation for model - https://tfhub.dev/google/movenet/multipose/lightning/1\n",
    "# Input images height and width should be multiple of 32\n",
    "# Transformed image should preserve original aspect ratio\n",
    "# Larger side should be multiples of 256 \n",
    "\n",
    "while capture.isOpened():\n",
    "    retval, frame = capture.read()\n",
    "\n",
    "    # Resize Image \n",
    "    img = frame.copy()\n",
    "    img = tf.expand_dims(img, axis=0)\n",
    "    height, width = keep_aspect_ratio_resizer(frame, 512)\n",
    "    img = tf.image.resize_with_pad(img, height, width)\n",
    "    input_img = tf.cast(img, dtype=tf.int32)\n",
    "\n",
    "\n",
    "    # Make Detections\n",
    "    output = movenet(input_img) # Shape: (1,6,56)\n",
    "    keypoints_and_scores = output[\"output_0\"].numpy()[:,:,:51].reshape((6,17,3)) # Shape: (6,17,3)\n",
    "\n",
    "    # Render keypoints and edges\n",
    "    draw_keypoints_and_edges(frame, keypoints_and_scores, 0.3)\n",
    "\n",
    "    if retval:\n",
    "        cv2.imshow(\"Multi Person Pose Detection\", frame)\n",
    "\n",
    "    pressed_key = cv2.waitKey(10)\n",
    "    if pressed_key == ord(\"q\") or pressed_key == ord(\"Q\"):\n",
    "        break\n",
    "\n",
    "capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "47331e52e889b4e351db732566d57bcba7f5f979e5e0b22c6ea30241b064d76d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
